{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing (RE)PPTIS simulations using an MSM approach\n",
    "This notebook contains an example workflow that can be used for estimating the crossing probability and pathlengths of a (RE)PPTIS simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint    # to print the vars of the pathensemble object\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Reading\n",
    "from tistools import read_inputfile, get_LMR_interfaces, read_pathensemble, get_weights\n",
    "from tistools import set_tau_distrib, set_tau_first_hit_M_distrib, cross_dist_distr, pathlength_distr\n",
    "from tistools import collect_tau, collect_tau1, collect_tau2, collect_taum\n",
    "from tistools import ACCFLAGS, REJFLAGS\n",
    "\n",
    "# REPPTIS analysis\n",
    "from tistools import get_lmr_masks, get_generation_mask, get_flag_mask, select_with_masks\n",
    "from tistools import unwrap_by_weight, running_avg_local_probs, get_local_probs, get_global_probs_from_dict, get_global_probs_from_local\n",
    "\n",
    "# MSM functions\n",
    "from tistools import construct_M\n",
    "from tistools import global_pcross_msm\n",
    "from tistools import mfpt_to_first_last_state, mfpt_to_absorbing_states, construct_tau_vector\n",
    "from tistools import create_labels_states, print_vector, print_all_tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the simulation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run this notebook\n",
    "- Set `indir` to the simulation directory (see the commented presets below).\n",
    "- Toggle `zero_minus_one` if you have a λ\n",
    "\n",
    "−1 interface.\n",
    "- Set `VERBOSE` to `True` if you want detailed per-pathensemble output; otherwise it stays concise.\n",
    "- Figures are controlled by `makefigs` later on.\n",
    "- The notebook assumes PyRETIS/REPPTIS output structure with numbered folders under `indir`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "indir = \"/mnt/0bf0c339-34bb-4500-a5fb-f3c2a863de29/DATA/APPTIS/simdata/infrepptis/\"  \n",
    "\n",
    "\n",
    "# Control verbosity and options\n",
    "VERBOSE = True          # Set True for detailed per-ensemble printing\n",
    "PRINT_PATHENSEMBLE = True  # Set True to pprint each pathensemble vars\n",
    "\n",
    "# zero_minus_one: True if lambda_-1 interface is set\n",
    "zero_minus_one = False\n",
    "\n",
    "inputfile = indir + \"/repptis.rst\"    # When using PyRETIS, the input file for REPPTIS simulations is a .rst file\n",
    "\n",
    "# Move to working directory\n",
    "os.chdir(indir)\n",
    "print(os.getcwd())\n",
    "\n",
    "# Set the ensemble folders and print them\n",
    "folders = sorted(glob.glob(indir + \"/0[0-9][0-9]\"))\n",
    "print(f\"Found {len(folders)} ensemble folders\")\n",
    "print(folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all input\n",
    "#===================\n",
    "interfaces, zero_left, timestep = read_inputfile(inputfile)\n",
    "LMR_interfaces, LMR_strings = get_LMR_interfaces(interfaces, zero_left)\n",
    "\n",
    "pathensembles = []\n",
    "for i, fol in enumerate(folders):\n",
    "    pe = read_pathensemble(fol + \"/pathensemble.txt\")\n",
    "    pe.set_name(fol)\n",
    "    pe.set_interfaces([LMR_interfaces[i], LMR_strings[i]])\n",
    "    if i == 0:\n",
    "        pe.set_zero_minus_one(zero_minus_one)   # TODO this is never used\n",
    "        pe.set_in_zero_minus(True)\n",
    "    if i == 1:\n",
    "        pe.set_in_zero_plus(True)\n",
    "\n",
    "    w, _ = get_weights(pe.flags, ACCFLAGS, REJFLAGS, verbose=False)\n",
    "    pe.set_weights(w)\n",
    "\n",
    "    if PRINT_PATHENSEMBLE:\n",
    "        print(\"#\" * 80)\n",
    "        print(fol)\n",
    "        print(\"pathensemble info:\")\n",
    "        pprint(vars(pe))\n",
    "    else:\n",
    "        print(f\"Loaded {fol} (ensemble {i})\")\n",
    "\n",
    "    # Read order parameters order.txt/order.npy into path ensemble object, or load from order.npy file.\n",
    "    # Saving order parameter files allows to speed up this notebook.\n",
    "    pe.set_orders(load=False, acc_only=True, save=True)        # first run: store .npy files\n",
    "    # pe.set_orders(load=True, acc_only=True, save=False)          # subsequent runs: read .npy files\n",
    "    # pe.set_orders(load=False, acc_only=True, save=False)       # if saving doesn't work\n",
    "\n",
    "    pathensembles.append(pe)\n",
    "\n",
    "print(f\"\\nLoaded {len(pathensembles)} pathensembles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regular (RE)PPTIS analysis using tistools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the REPPTIS simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip on output volume**: Set `VERBOSE = True` (in the setup cell above) to see detailed per-ensemble diagnostics. With `VERBOSE = False`, only compact summaries are printed to keep the log tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis output is saved to the data dictionary.\n",
    "data = {}\n",
    "for i, pe in enumerate(pathensembles):\n",
    "    if i == 0:\n",
    "        data[i] = {}\n",
    "        continue  # [0-] is not used for Pcross calculations\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(f\"doing pathensemble {i}\")\n",
    "\n",
    "    # Classify the paths according to their path type.\n",
    "    pathtypes = (\"LML\", \"LMR\", \"RML\", \"RMR\")\n",
    "    pathtype_cycles = {}\n",
    "    for ptype in pathtypes:\n",
    "        pathtype_cycles[ptype] = unwrap_by_weight(\n",
    "            (pe.lmrs == ptype).astype(int), pe.weights\n",
    "        )\n",
    "    \n",
    "    # Running average analysis: [\"running\"]\n",
    "    data[i] = {}\n",
    "    data[i][\"running\"] = {}\n",
    "    data[i][\"running\"][\"plocal\"] = {}\n",
    "    for (ptype, p_loc) in zip(\n",
    "        pathtypes,\n",
    "        running_avg_local_probs(pathtype_cycles, pe.weights, tr=False)\n",
    "    ):\n",
    "        data[i][\"running\"][\"plocal\"][ptype] = p_loc\n",
    "\n",
    "    # Analysis using all data: [\"full\"]\n",
    "    plocfull = get_local_probs(pe, tr=False)\n",
    "    data[i][\"full\"] = {}\n",
    "    for ptype in pathtypes:\n",
    "        data[i][\"full\"][ptype] = plocfull[ptype]\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(f\"Finished ensemble {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate pathlength distribution figures, as in PyRETIS reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, pe in enumerate(pathensembles):\n",
    "    upe = pe.unify_pe()\n",
    "    # Pathlength distribution\n",
    "    data[i][\"pathlengths\"] = pathlength_distr(upe)  # these might be used later or not! TODO\n",
    "        \n",
    "#=======================================\n",
    "# make figures\n",
    "makefigs = True \n",
    "if makefigs:\n",
    "    for i, pe in enumerate(pathensembles):     \n",
    "        if i == 0:\n",
    "            continue\n",
    "        # Cross distances distribution\n",
    "        L, M, R, lmlpercs, lmllambs, rmrpercs, rmrlambs = cross_dist_distr(pe)\n",
    "        fig,ax = plt.subplots()\n",
    "        ax.plot(lmllambs, lmlpercs, lw=1, c=\"g\")\n",
    "        ax.plot(rmrlambs, rmrpercs, lw=1, c=\"r\")\n",
    "        for lamb in (L,M,R):\n",
    "            ax.axvline(lamb, color='k', linestyle='--', lw = 0.5)\n",
    "        ax.set_xlabel('Cross distance')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title(\"Ensemble {}. L = {}, M = {}, R = {}\".format(\n",
    "            pe.name, L, M, R))\n",
    "        ax.set_ylim(0)\n",
    "        fig.savefig(f\"pathensemble_{i}_crossdist.pdf\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Pathlength distribution      \n",
    "        for ptype in pathtypes:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(data[i][\"pathlengths\"][ptype][\"bin_centers\"], \n",
    "                data[i][\"pathlengths\"][ptype][\"hist\"])\n",
    "            ax.set_xlabel('Pathlength')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(f\"{np.sum(data[i]['pathlengths'][ptype]['hist'])} \" + \\\n",
    "                         f\"{ptype} paths. \")\n",
    "            ax.legend([f\"mean = {data[i]['pathlengths'][ptype]['mean']:.2f}, \" + \\\n",
    "                          f\"std = {data[i]['pathlengths'][ptype]['std']:.2f}\"])\n",
    "            fig.savefig(f\"pathensemble_{i}_pathlength_{ptype}.pdf\")\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Pcross using in-house functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global crossing probabilities (no error analysis)  \n",
    "psfull = []\n",
    "for i in range(1, len(pathensembles)):   # do not use the 0- ensemble\n",
    "    psfull.append({\"LMR\": data[i][\"full\"][\"LMR\"], \n",
    "               \"RML\": data[i][\"full\"][\"RML\"], \n",
    "               \"RMR\": data[i][\"full\"][\"RMR\"],\n",
    "               \"LML\": data[i][\"full\"][\"LML\"]})\n",
    "\n",
    "Pminfull, Pplusfull, Pcrossfull = get_global_probs_from_dict(psfull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a figure of the global crossing probabilities\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_yscale(\"log\")\n",
    "# ax.plot(Pcrossfull, \"o\", c = \"r\")\n",
    "ax.errorbar(interfaces, Pcrossfull, fmt=\"-o\", c = \"b\", ecolor=\"r\", capsize=6)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"OP\")\n",
    "ax.set_ylabel(r\"$P_A(\\lambda_i|\\lambda_A)$\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(\"Global_probs.pdf\")\n",
    "\n",
    "print(\"This should be the same as the repptis_report.pdf value:\", Pcrossfull[-1])\n",
    "print(\"which is the case!\")\n",
    "print(Pcrossfull)\n",
    "print([Pcrossfull[i]/Pcrossfull[i-1] for i in range(1,len(Pcrossfull))])\n",
    "print(\"Here, the load immediately disappeared. For a simulation where this is\")\n",
    "print(\"not the case, the above code should be adapted a little bit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct lists of the local probs\n",
    "\n",
    "# Or we can use the get_global_probs_from_local function, using lists of the local probs\n",
    "# These do not use the 0- ensemble\n",
    "pmps = [data[i][\"full\"][\"LMR\"] for i in range(1,len(pathensembles))]\n",
    "pmms = [data[i][\"full\"][\"LML\"] for i in range(1,len(pathensembles))]\n",
    "ppps = [data[i][\"full\"][\"RMR\"] for i in range(1,len(pathensembles))]\n",
    "ppms = [data[i][\"full\"][\"RML\"] for i in range(1,len(pathensembles))]\n",
    "a,b,c = get_global_probs_from_local(pmps, pmms, ppps, ppms)\n",
    "print(\"This should be the same as the repptis_report.pdf value:\", c[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis using the MSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct transition matrix M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interfaces)\n",
    "N = len(interfaces)\n",
    "NS = 4*N-5\n",
    "print(\"N\", N)\n",
    "# print(\"len pmms\", len(pmms)) # TODO INCLUDE?\n",
    "print(\"NS\", NS)\n",
    "\n",
    "labels1, labels2 = create_labels_states(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VERBOSE:\n",
    "    print(\"mm\", pmms)\n",
    "    print(\"mp\", pmps)\n",
    "    print(\"pm\", ppms)\n",
    "    print(\"pp\", ppps)\n",
    "    print(\"sum\", np.array(pmms) + np.array(pmps))\n",
    "    print(\"sum\", np.array(ppms) + np.array(ppps))\n",
    "\n",
    "M = construct_M(pmms, pmps, ppms, ppps, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can print the transition matrix M and check that all rows sum to 1.\n",
    "if VERBOSE:\n",
    "    print(\"M\")\n",
    "    print(\"shape\", M.shape)\n",
    "    print(\"sum prob in rows\", np.sum(M, axis=1))\n",
    "    print(M)\n",
    "else:\n",
    "    print(\"Transition matrix built. Set VERBOSE=True to print matrix and row sums.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at this Markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VERBOSE:\n",
    "    vals, vecs = np.linalg.eig(M)\n",
    "    print(vals)\n",
    "    vals, vecs = np.linalg.eig(M.T)\n",
    "    print(vals)\n",
    "    pprint(M)\n",
    "else:\n",
    "    vals, _ = np.linalg.eig(M)\n",
    "    print(f\"Eigenvalues (summary): min={np.min(vals):.3f}, max={np.max(vals):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if VERBOSE:\n",
    "    print(\"what if chain propagates\")\n",
    "    print(\"A[0,:]\")\n",
    "    # check stationary behavior\n",
    "    A = M\n",
    "    for n in range(10):\n",
    "        A = np.dot(A, M)\n",
    "        print(A[0, :])\n",
    "        print(np.sum(A[0, :]))  # is 1 indeed\n",
    "else:\n",
    "    print(\"Skipping propagation printout (set VERBOSE=True to inspect A^n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pcross with MSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Z and Y vectors\n",
    "\n",
    "z1, z2, y1, y2 = global_pcross_msm(M)\n",
    "print(\"Z\")\n",
    "print_vector(z1, labels1)\n",
    "print_vector(z2, labels2)\n",
    "print(\"Y\")\n",
    "print_vector(y1, labels1)\n",
    "print_vector(y2, labels2)\n",
    "print(\"\\nGlobal crossing probability: \", y1[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pathlength analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path ensemble properties\n",
    "#==================================\n",
    "for i,fol in enumerate(folders):\n",
    "    print(i)\n",
    "    print(\"Calculating path lengths.\")\n",
    "    set_tau_distrib(pathensembles[i])\n",
    "    print(\"Done.\")\n",
    "\n",
    "    if True:\n",
    "        print(\"Calculating first hitting lengths to middle interface\")\n",
    "        set_tau_first_hit_M_distrib(pathensembles[i])\n",
    "        print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional information\n",
    "#==================================\n",
    "# Average path lengths per ensemble for each path type\n",
    "print(indir[-20:])\n",
    "pathtypes = (\"LML\", \"LMR\", \"RMR\", \"RML\", \"LM*\", \"*M*\", \"***\", \"RM*\", \"L**\", \"**R\", \"R**\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"AVERAGE PATH LENGTHS BY ENSEMBLE AND PATH TYPE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, pe in enumerate(pathensembles):\n",
    "    if not VERBOSE:\n",
    "        print(f\"Ensemble {i}: accepted paths = {np.sum(np.isin(pe.flags, ACCFLAGS))}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nEnsemble {i} ({pe.name}):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get accepted paths only\n",
    "    accepted_mask = np.isin(pe.flags, ACCFLAGS)\n",
    "    total_accepted_count = np.sum(accepted_mask)\n",
    "    \n",
    "    if total_accepted_count > 0:\n",
    "        # Calculate weighted average for all accepted paths\n",
    "        accepted_lengths = pe.lengths[accepted_mask]\n",
    "        accepted_weights = pe.weights[accepted_mask]\n",
    "        total_weighted_avg = np.average(accepted_lengths, weights=accepted_weights)\n",
    "        \n",
    "        print(f\"  All accepted paths: {total_weighted_avg:8.2f} (n={total_accepted_count:4d}, weighted)\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    for ptype in pathtypes:\n",
    "        mask = (pe.lmrs == ptype) & accepted_mask\n",
    "        if np.any(mask):\n",
    "            lengths = pe.lengths[mask]\n",
    "            weights = pe.weights[mask]\n",
    "            weighted_avg = np.average(lengths, weights=weights)\n",
    "            count = np.sum(mask)\n",
    "            print(f\"  {ptype:4s}: {weighted_avg:8.2f} (n={count:4d}, weighted)\")\n",
    "        else:\n",
    "            print(f\"  {ptype:4s}: {0:8.2f} (n={0:4d}, weighted)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute taus for pathlength analysis\n",
    "tau_mm, tau_mp, tau_pm, tau_pp = collect_tau(pathensembles)\n",
    "tau1_mm, tau1_mp, tau1_pm, tau1_pp = collect_tau1(pathensembles)\n",
    "tau2_mm, tau2_mp, tau2_pm, tau2_pp = collect_tau2(pathensembles)\n",
    "taum_mm, taum_mp, taum_pm, taum_pp = collect_taum(pathensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at computed taus\n",
    "if VERBOSE:\n",
    "    print(\"tau\")\n",
    "    print_all_tau(pathensembles, tau_mm, tau_mp, tau_pm, tau_pp)\n",
    "    print(\"\\ntau1\")\n",
    "    print_all_tau(pathensembles, tau1_mm, tau1_mp, tau1_pm, tau1_pp)\n",
    "    print(\"\\ntaum\")\n",
    "    print_all_tau(pathensembles, taum_mm, taum_mp, taum_pm, taum_pp)\n",
    "    print(\"\\ntau2\")\n",
    "    print_all_tau(pathensembles, tau2_mm, tau2_mp, tau2_pm, tau2_pp)\n",
    "else:\n",
    "    print(\"Tau diagnostics suppressed (set VERBOSE=True for details)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO include prints?\n",
    "tau  = construct_tau_vector(N, NS, tau_mm, tau_mp, tau_pm, tau_pp)\n",
    "tau1 = construct_tau_vector(N, NS, tau1_mm, tau1_mp, tau1_pm, tau1_pp)\n",
    "taum = construct_tau_vector(N, NS, taum_mm, taum_mp, taum_pm, taum_pp)\n",
    "tau2 = construct_tau_vector(N, NS, tau2_mm, tau2_mp, tau2_pp)\n",
    "tau_m = tau - tau1 - tau2  # yes, this is the same thing as taum\n",
    "\n",
    "if VERBOSE:\n",
    "    print(\"tau\")\n",
    "    print(tau)\n",
    "    print(\"\\n\")\n",
    "    print(\"tau1\")\n",
    "    print(tau1)\n",
    "    print(\"taum\")\n",
    "    print(taum)\n",
    "    print(\"tau2\")\n",
    "    print(tau2)\n",
    "    print(\"\\n\")\n",
    "    print(\"tau = tau1+taum+tau2 => difference is\", np.sum((tau - tau1 - taum - tau2) ** 2))\n",
    "else:\n",
    "    diff = np.sum((tau - tau1 - taum - tau2) ** 2)\n",
    "    print(f\"Tau vectors built. Consistency diff: {diff:.3e} (set VERBOSE=True to inspect)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Flux calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect tau for [0+]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct g and h vectors\n",
    "g1, g2, h1, h2 = mfpt_to_first_last_state(M, np.nan_to_num(tau1), np.nan_to_num(tau_m), np.nan_to_num(tau2)) #, doprint=True)\n",
    "print(\"G\")\n",
    "print_vector(g1, labels1)\n",
    "print_vector(g2, labels2)\n",
    "print(\"H\")\n",
    "print_vector(h1, labels1)\n",
    "print_vector(h2, labels2)\n",
    "print(\"\\ntau [0+]: \", h1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flux = 1/(tau[0]+h1[0][0])\n",
    "print(tau[0], h1[0][0])\n",
    "dt = 0.002 # Change if needed\n",
    "sc = 10\n",
    "flux /= (dt*sc)\n",
    "print(flux*(dt*sc), \"1/time\")\n",
    "print(flux, \"1/ps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The rate constant\n",
    "We can compute an accurate rate constant using only our MSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate constant = flux * Pcross\n",
    "\n",
    "print(\"The rate constant k is: \", flux*y1[0][0]/dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Direct rate computation via $\\tau_{\\mathcal{A},1}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct g and h vectors\n",
    "absor = np.array([NS - 1])\n",
    "kept = np.array([i for i in range(NS) if i not in absor])\n",
    "\n",
    "g1, g2, h1, h2 = mfpt_to_absorbing_states(M, np.nan_to_num(tau1), np.nan_to_num(tau_m), np.nan_to_num(tau2), absor, kept, remove_initial_m=False) #, doprint=True)\n",
    "print(\"G\")\n",
    "print_vector(g1, labels1[-1])\n",
    "print_vector(g2, [labels1[0]] + labels2)\n",
    "print(\"H\")\n",
    "print_vector(h1, labels1[-1])\n",
    "print_vector(h2, [labels1[0]] + labels2)\n",
    "print(\"interesting\")\n",
    "print(h2[0])\n",
    "mfpt = h2[0][0]  # tau_A,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_flux_pcross = flux * y1[0][0] / dt\n",
    "k_mfpt = 1 / (mfpt * dt)\n",
    "\n",
    "print(f\"Rate constant from P_cross × flux: {k_flux_pcross:.10e} [1/ps]\")\n",
    "print(f\"Rate constant from MFPT:           {k_mfpt:.10e} [1/ps]\")\n",
    "print(f\"Relative difference:               {abs(k_flux_pcross - k_mfpt)/k_mfpt*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pastime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
